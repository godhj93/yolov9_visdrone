{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from models.yolo import DetectionModel\n",
    "import torch\n",
    "from utils.general import intersect_dicts\n",
    "from utils_decomposition import tucker_decomposition_conv_layer\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=10\n",
      "Overriding model.yaml anchors with anchors=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      "  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      "  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
      " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
      " 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
      " 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 22      [15, 18, 21]  1   5498350  models.yolo.DDetect                     [10, [256, 512, 512]]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 256, 32, 32])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 32, 32])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 32, 32])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 16, 16])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 16, 16])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 16, 16])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 8, 8])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 8, 8])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 8, 8])\u001b[0m\n",
      "\u001b[31mInput is (1, 3, 640, 640), It is important for QAT export, normal training is not realted to this.\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 256, 80, 80])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 80, 80])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 80, 80])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 40, 40])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 40, 40])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 40, 40])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 20, 20])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 20, 20])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 20, 20])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gelan-c summary: 621 layers, 25444782 parameters, 25444766 gradients, 128.7 GFLOPs\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "Overriding model.yaml anchors with anchors=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      "  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      "  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
      " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
      " 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
      " 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 256, 4, 4])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 4, 4])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 4, 4])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 2, 2])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 2, 2])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 2, 2])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 1, 1])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 1, 1])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 1, 1])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22      [15, 18, 21]  1   5498350  models.yolo.DDetect                     [10, [256, 512, 512]]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 256, 32, 32])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 32, 32])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 32, 32])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 16, 16])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 16, 16])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 16, 16])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 8, 8])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 8, 8])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 8, 8])\u001b[0m\n",
      "\u001b[31mInput is (1, 3, 640, 640), It is important for QAT export, normal training is not realted to this.\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 256, 80, 80])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 80, 80])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 80, 80])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 40, 40])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 40, 40])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 40, 40])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 20, 20])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 20, 20])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 20, 20])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gelan-c summary: 621 layers, 25444782 parameters, 25444766 gradients, 128.7 GFLOPs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 256, 4, 4])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 4, 4])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 4, 4])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 2, 2])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 2, 2])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 2, 2])\u001b[0m\n",
      "\u001b[31mx shape: 3\u001b[0m\n",
      "\u001b[31mInput shape: torch.Size([1, 512, 1, 1])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 64, 1, 1])\u001b[0m\n",
      "\u001b[31mOutput shapetorch.Size([1, 10, 1, 1])\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = torch.rand(1, 3, 640, 640).to(device)\n",
    "Model = DetectionModel\n",
    "model = Model(cfg='models/detect/gelan-c.yaml', ch=3, nc=10, anchors=3).to(device)  # create\n",
    "model_original = Model(cfg='models/detect/gelan-c.yaml', ch=3, nc=10, anchors=3).to(device)  # create\n",
    "exclude = ['anchor'] # exclude keys\n",
    "weight = 'runs/train/yolov9-c/weights/yolov9-c-converted.pt'\n",
    "ckpt = torch.load(weight)\n",
    "csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "model.load_state_dict(csd, strict=False)  # load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.1.conv Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.2.cv2.0.m.0.cv1.conv1.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv2.0.m.0.cv2.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv2.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv3.0.m.0.cv1.conv1.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv3.0.m.0.cv2.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv3.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.3.cv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.4.cv2.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv2.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv2.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv3.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv3.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv3.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.5.cv1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.6.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.7.cv1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.8.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv2.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv2.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv2.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv3.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv3.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv3.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.16.cv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.18.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.19.cv1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.21.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv2.0.0.conv Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv2.1.0.conv Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv2.2.0.conv Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.0.0.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.0.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.1.0.conv Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.1.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.2.0.conv Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "def get_conv_layers_with_kernel_size_3(model):\n",
    "    conv_layers = []\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Conv2d) and layer.kernel_size == (3, 3) and layer.groups == 1:\n",
    "            conv_layers.append((name, layer))\n",
    "    return conv_layers\n",
    "\n",
    "conv_layers = get_conv_layers_with_kernel_size_3(model)\n",
    "\n",
    "for name, layer in conv_layers:\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.1.conv Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.2.cv2.0.m.0.cv1.conv1.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv2.0.m.0.cv2.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv2.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv3.0.m.0.cv1.conv1.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv3.0.m.0.cv2.conv Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.2.cv3.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.3.cv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.4.cv2.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv2.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv2.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv3.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv3.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.4.cv3.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.5.cv1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.6.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.6.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.7.cv1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.8.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.8.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.12.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv2.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv2.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv2.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv3.0.m.0.cv1.conv1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv3.0.m.0.cv2.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.15.cv3.1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.16.cv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.18.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.18.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.19.cv1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "model.21.cv2.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv2.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv3.0.m.0.cv1.conv1.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv3.0.m.0.cv2.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.21.cv3.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv2.0.0.conv Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv2.1.0.conv Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv2.2.0.conv Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.0.0.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.0.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.1.0.conv Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.1.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.2.0.conv Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "model.22.cv3.2.1.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "# 각 합성곱 레이어에 대해 텐서 분해 수행\n",
    "for idx, (name, conv_layer) in enumerate(conv_layers):\n",
    "    print(name, conv_layer)\n",
    "    # factorized_layers는 분해된 레이어들\n",
    "    # try:\n",
    "    factorized_layers = tucker_decomposition_conv_layer(conv_layer.cpu())\n",
    "    # except:\n",
    "    #     print(\"Error in Tucker decomposition\")\n",
    "    #     continue\n",
    "    pointer = model\n",
    "    names = name.split('.')\n",
    "    for n in names[:-1]:  # 마지막 이름을 제외하고 모든 중간 모듈을 참조\n",
    "        pointer = getattr(pointer, n)\n",
    "    setattr(pointer, names[-1], factorized_layers)  # 마지막 모듈에 factorized_layers 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def compare_parameters(model1, model2):\n",
    "    model1_params = count_parameters(model1)\n",
    "    model2_params = count_parameters(model2)\n",
    "    \n",
    "    # print(f\"{model1_params:,}, {model2_params:,}\")\n",
    "    return model1_params, model2_params\n",
    "\n",
    "def plot_parameters_comparison(model1_params, model2_params, model1_name='Model1', model2_name='Model2'):\n",
    "    max_params = max(model1_params, model2_params)\n",
    "\n",
    "    model1_percentage = (model1_params / max_params) * 100\n",
    "    model2_percentage = (model2_params / max_params) * 100\n",
    "    \n",
    "    print(f\"Model1 Param: {model1_params:,}\")\n",
    "    print(f\"Model2 Param: {model2_params:,}\")\n",
    "    print(f\"Parameter reduction: {(model1_params - model2_params):,}, {(1-(model2_params/model1_params))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 Param: 25,444,782\n",
      "Model2 Param: 10,117,310\n",
      "Parameter reduction: 15,327,472, 60.24%\n"
     ]
    }
   ],
   "source": [
    "plot_parameters_comparison(*compare_parameters(model_original, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the decomposed model\n",
    "torch.save(model, weight.replace('.pt', '_decomposed.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
